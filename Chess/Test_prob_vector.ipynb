{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test prob vector.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TWdjqqM0Abz"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib\n",
        "# matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import datetime\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wibiwnm80XuP"
      },
      "source": [
        "from torch.nn import MSELoss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x-ydClY0ap3"
      },
      "source": [
        "# get data from Oscar, might need to rewrite to fit the data structure he has\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data, target, transform=None):\n",
        "        self.data = torch.from_numpy(data).float()\n",
        "        self.target = torch.from_numpy(target).long()\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "            \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "numpy_data = np.random.randn(100,112, 8, 8) # 100 samples, image size = 112 x 8 x 8\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcrbL-am0mQd"
      },
      "source": [
        "numpy_target = np.random.randn(100, 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0yhoqky0t_D"
      },
      "source": [
        "from scipy.special import softmax\n",
        "numpy_target = softmax(numpy_target, axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77P9TqhL0_Mw"
      },
      "source": [
        "action_size = 200\n",
        "num_resblock = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gdXLea0guW"
      },
      "source": [
        "dataset = MyDataset(numpy_data, numpy_target)\n",
        "loader = DataLoader(dataset, batch_size=5, shuffle=True, num_workers=2, pin_memory=False)  # Running on CPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBoMlwGP1Qp1"
      },
      "source": [
        "# convblock for doing convolutional work\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.action_size = action_size\n",
        "        self.conv1 = nn.Conv2d(112, 256, 3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(256)\n",
        "\n",
        "    def forward(self, s):\n",
        "        s = s.view(-1, 112, 8, 8)  # batch_size x channels x board_x x board_y\n",
        "        s = F.relu(self.bn1(self.conv1(s)))\n",
        "        return s\n",
        "\n",
        "\n",
        "# Resblock to do residual block: x + conv output (x)\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, inplanes=256, planes=256, stride=1, downsample=None):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = F.relu(self.bn1(out))\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += residual\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# final FC layers\n",
        "# get more layer in the last round + flatten\n",
        "class OutBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OutBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(256, 1, kernel_size=1) # value head\n",
        "        self.bn = nn.BatchNorm2d(1)\n",
        "        self.fc1 = nn.Linear(8*8, 100)\n",
        "        self.fc2 = nn.Linear(100, action_size)\n",
        "        \n",
        "        # self.conv1 = nn.Conv2d(256, 128, kernel_size=1) # policy head\n",
        "        # self.bn1 = nn.BatchNorm2d(128)\n",
        "        # self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "        # self.fc = nn.Linear(8*8*128, 8*8*73)\n",
        "    \n",
        "    def forward(self,s):\n",
        "        v = F.relu(self.bn(self.conv(s))) # value head\n",
        "        v = v.view(-1, 8*8)  # batch_size X channel X height X width\n",
        "        v = F.relu(self.fc1(v))\n",
        "        v = F.relu(self.fc2(v))\n",
        "        \n",
        "        # p = F.relu(self.bn1(self.conv1(s))) # policy head\n",
        "        # p = p.view(-1, 8*8*128)\n",
        "        # p = self.fc(p)\n",
        "        # p = self.logsoftmax(p).exp()\n",
        "        # print(\"haha\", v.shape)\n",
        "        return v\n",
        "    \n",
        "# stacking conv block + a bunch of res block + out block\n",
        "class ChessNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ChessNet, self).__init__()\n",
        "        self.conv = ConvBlock()\n",
        "        for block in range(num_resblock):\n",
        "            setattr(self, \"res_%i\" % block,ResBlock())\n",
        "        self.outblock = OutBlock()\n",
        "    \n",
        "    def forward(self,s):\n",
        "        s = self.conv(s)\n",
        "        for block in range(num_resblock):\n",
        "            s = getattr(self, \"res_%i\" % block)(s)\n",
        "        s = self.outblock(s)\n",
        "        return s\n",
        "        \n",
        "    \n",
        "# training\n",
        "def train(net, dataset, epoch_start=0, epoch_stop=20, cpu=0):\n",
        "    torch.manual_seed(cpu)\n",
        "    cuda = torch.cuda.is_available()\n",
        "    net.train()\n",
        "    criterion = MSELoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.003)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,400], gamma=0.2)\n",
        "    \n",
        "    # train_set = board_data(dataset)\n",
        "    # train_loader = DataLoader(train_set, batch_size=30, shuffle=True, num_workers=0, pin_memory=False)\n",
        "    train_loader = dataset\n",
        "    losses_per_epoch = []\n",
        "    for epoch in range(epoch_start, epoch_stop):\n",
        "        scheduler.step()\n",
        "        total_loss = 0.0\n",
        "        losses_per_batch = []\n",
        "        for i,data in enumerate(train_loader,0):\n",
        "            state, value = data\n",
        "            # print(\"loll\", value.shape)\n",
        "            if cuda:\n",
        "                state, value = state.cuda().float(), value.cuda().float()\n",
        "            optimizer.zero_grad()\n",
        "            value_pred = net(state) # policy_pred = torch.Size([batch, 4672]) value_pred = torch.Size([batch, 1])\n",
        "            # print(value_pred, value)\n",
        "            # print(value_pred.shape, value.shape)\n",
        "            loss = criterion(value_pred, value)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            losses_per_batch.append(loss.item())\n",
        "        try:\n",
        "          losses_per_epoch.append(np.mean(losses_per_batch))\n",
        "        except:\n",
        "          losses_per_epoch.append(0.1)\n",
        "        if len(losses_per_epoch) > 100:\n",
        "            if abs(sum(losses_per_epoch[-4:-1])/3-sum(losses_per_epoch[-16:-13])/3) <= 0.01:\n",
        "                break\n",
        "\n",
        "    plt.figure(figsize=(12,8))\n",
        "    # ax = fig.add_subplot(222)\n",
        "    plt.scatter([e for e in range(1,epoch_stop+1,1)], losses_per_epoch)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss per batch\")\n",
        "    plt.title(\"Loss vs Epoch\")\n",
        "    print('Finished Training')\n",
        "    plt.savefig(os.path.join(\"./\", \"Loss_vs_Epoch_%s.png\" % datetime.datetime.today().strftime(\"%Y-%m-%d\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoZYQV1A1UyF",
        "outputId": "9b5e1789-2769-49ec-fe36-cb91cb175f9c"
      },
      "source": [
        "# from alpha_net import ChessNet, train\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def train_chessnet(net_to_train= None ,save_as= \"weights.pth.tar\"):\n",
        "    # gather data\n",
        "    # data_path = \"./datasets/iter1/\"\n",
        "    # datasets = []\n",
        "    # for idx,file in enumerate(os.listdir(data_path)):\n",
        "    #     filename = os.path.join(data_path,file)\n",
        "    #     with open(filename, 'rb') as fo:\n",
        "    #         datasets.extend(pickle.load(fo, encoding='bytes'))\n",
        "    \n",
        "    # data_path = \"./datasets/iter0/\"\n",
        "    # for idx,file in enumerate(os.listdir(data_path)):\n",
        "    #     filename = os.path.join(data_path,file)\n",
        "    #     with open(filename, 'rb') as fo:\n",
        "    #         datasets.extend(pickle.load(fo, encoding='bytes'))\n",
        "    \n",
        "    # datasets = np.array(datasets)\n",
        "    \n",
        "    # train net\n",
        "\n",
        "    datasets = loader\n",
        "    net = ChessNet()\n",
        "    cuda = torch.cuda.is_available()\n",
        "    if cuda:\n",
        "        net.cuda()\n",
        "\n",
        "    if net_to_train:\n",
        "        current_net_filename = os.path.join(\"./model_data/\",\\\n",
        "                                        net_to_train)\n",
        "        checkpoint = torch.load(current_net_filename)\n",
        "        net.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    train(net,datasets)\n",
        "    # save results\n",
        "    # torch.save({'state_dict': net.state_dict()}, os.path.join(\"./model/\",\\\n",
        "    #                                 save_as))\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    train_chessnet()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNN0nawZ1cJW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}